{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Dataset\n",
    "\n",
    "The dataset used in this project, called **'Garbage Image Dataset'**, is available [here](https://www.kaggle.com/datasets/farzadnekouei/trash-type-image-dataset/). There are **2.527 images** distributed among six distinct classes:\n",
    "* Cardboard: **403 images**;\n",
    "* Glass: **501 images**;\n",
    "* Metal: **410 images**;\n",
    "* Paper: **594 images**;\n",
    "* Plastic: **482 images**;\n",
    "* Trash: **137 images**.\n",
    "\n",
    "Before to start the exploration of the dataset, this notebook aims to divide the images in subsets to train, validate and test. The technique used is cross-validation of the type **hold-out** (70% train / 20% validation /10% test).\n",
    "\n",
    "Furthermore, in order to build the training and validation partitions, the frameworks **TensorFlow** and **Keras** will be used in the next notebook to prepare these images correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAGGLE_DATASET_TAG = 'farzadnekouei/trash-type-image-dataset'\n",
    "\n",
    "CURRENT_PATH = os.getcwd()\n",
    "RAW_DATASET_PATH = os.path.join(CURRENT_PATH, 'datasets', 'RAW_trash_type_dataset')\n",
    "DATASET_PATH = os.path.join(CURRENT_PATH, 'datasets', 'trash_type_dataset')\n",
    "\n",
    "VALIDATION_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the remote dataset to local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.3)\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/farzadnekouei/trash-type-image-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40.9M/40.9M [00:02<00:00, 20.4MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting model files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset path: C:\\Users\\Gabriel\\.cache\\kagglehub\\datasets\\farzadnekouei\\trash-type-image-dataset\\versions\\1\\TrashType_Image_Dataset\n",
      "Imported dataset: c:\\Users\\Gabriel\\OneDrive\\Documentos\\projetos\\gabriel\\fine-tuning-cnns-example\\datasets\\RAW_trash_type_dataset\n"
     ]
    }
   ],
   "source": [
    "assert not os.path.isdir(RAW_DATASET_PATH) and not os.path.isdir(DATASET_PATH), \"Dataset already exists.\"\n",
    "\n",
    "# Downloading the dataset \n",
    "cache_path = kagglehub.dataset_download(KAGGLE_DATASET_TAG, force_download=True)\n",
    "raw_dataset_path = os.path.join(cache_path, os.listdir(cache_path)[0])\n",
    "print(f'Original dataset path: {raw_dataset_path}')\n",
    "\n",
    "# Creating a path to move the dataset to the current path\n",
    "os.makedirs(RAW_DATASET_PATH)\n",
    "\n",
    "# Move each folder of the dataset to the new path\n",
    "for folder in os.listdir(raw_dataset_path):\n",
    "    folder_path = os.path.join(raw_dataset_path, folder)\n",
    "    target_folder_path = os.path.join(RAW_DATASET_PATH, folder)\n",
    "    os.rename(folder_path, target_folder_path)\n",
    "\n",
    "print(f\"Imported dataset: {RAW_DATASET_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Train/Validation/Test partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_directory(path):\n",
    "    files = os.listdir(path)\n",
    "    files_and_paths = [(file, os.path.join(path, file)) for file in files]\n",
    "    return files_and_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing all the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2527 entries, 0 to 2526\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   class_name  2527 non-null   object\n",
      " 1   class_path  2527 non-null   object\n",
      " 2   image_name  2527 non-null   object\n",
      " 3   image_path  2527 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 79.1+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset = {\n",
    "    'class_name': [],\n",
    "    'class_path': [],\n",
    "    'image_name': [],\n",
    "    'image_path': []\n",
    "}\n",
    "\n",
    "for class_name, class_path in read_directory(RAW_DATASET_PATH):\n",
    "    for image, image_path in read_directory(class_path):\n",
    "        dataset['class_name'].append(class_name)\n",
    "        dataset['class_path'].append(class_path)\n",
    "        dataset['image_name'].append(image)\n",
    "        dataset['image_path'].append(image_path)\n",
    "\n",
    "dataset = pd.DataFrame.from_dict(dataset)\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many classes the dataset have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 classes: ['cardboard' 'glass' 'metal' 'paper' 'plastic' 'trash']\n"
     ]
    }
   ],
   "source": [
    "classes = dataset['class_name'].unique()\n",
    "print(f\"{len(classes)} classes: {classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(dataset)\n",
    "train_validation_dataset, test_dataset = train_test_split(dataset, \n",
    "                                                          stratify=dataset['class_name'], \n",
    "                                                          test_size=int(TEST_SPLIT*total),\n",
    "                                                          shuffle=True)\n",
    "train_dataset, validation_dataset = train_test_split(train_validation_dataset, \n",
    "                                                     stratify=train_validation_dataset['class_name'],\n",
    "                                                     test_size=int(VALIDATION_SPLIT*total),\n",
    "                                                     shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size =      2527 images\n",
      "Train size =      1770 images [70%]\n",
      "Validation size = 505 images [20%]\n",
      "Test size =       252 images [10%]\n"
     ]
    }
   ],
   "source": [
    "train_size = len(train_dataset)\n",
    "val_size = len(validation_dataset)\n",
    "test_size = len(test_dataset)\n",
    "\n",
    "print(f\"Total size =      {total} images\")\n",
    "print(f\"Train size =      {train_size} images [{(train_size/total)*100:.0f}%]\")\n",
    "print(f\"Validation size = {val_size} images [{(val_size/total)*100:.0f}%]\")\n",
    "print(f\"Test size =       {test_size} images [{(test_size/total)*100:.0f}%]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copying images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_image(source_path: str, target_path: str):\n",
    "    os.rename(source_path, target_path)\n",
    "\n",
    "def copying_dataset_to_partition_dir(dataset: pd.DataFrame, \n",
    "                                      title: str,\n",
    "                                      output_path: str):\n",
    "    output_path = os.path.join(output_path, title)\n",
    "    assert not os.path.isdir(output_path), f'\"{output_path}\" directory already exists.'\n",
    "\n",
    "    # Creating the dataset partition folder\n",
    "    os.makedirs(output_path)\n",
    "    \n",
    "    for _, value in dataset.iterrows():\n",
    "        image_path = value['image_path']\n",
    "        class_name = value['class_name']\n",
    "        image = value['image_name']\n",
    "\n",
    "        # Creating the class folder it not exists\n",
    "        image_folder_path = os.path.join(output_path, class_name)\n",
    "        if not os.path.isdir(image_folder_path):\n",
    "            os.makedirs(image_folder_path)\n",
    "\n",
    "        # Copying the image\n",
    "        image_output_path = os.path.join(image_folder_path, image)\n",
    "        copy_image(image_path, image_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "copying_dataset_to_partition_dir(dataset=train_dataset, \n",
    "                                  title=\"train\", \n",
    "                                  output_path=DATASET_PATH)\n",
    "copying_dataset_to_partition_dir(dataset=validation_dataset, \n",
    "                                  title=\"validation\", \n",
    "                                  output_path=DATASET_PATH)\n",
    "copying_dataset_to_partition_dir(dataset=test_dataset, \n",
    "                                  title=\"test\", \n",
    "                                  output_path=DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, remove the original folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(RAW_DATASET_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
