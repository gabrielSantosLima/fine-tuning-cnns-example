{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning of CNN models\n",
    "\n",
    "In order to transfer learning from a pre-trained models available in [Keras](https://keras.io/api/applications/) to solve specific problems, this project shows how to apply a famous machine learning technique called **fine-tuning**.\n",
    "\n",
    "**Problem: [Domestic Garbage Detection](https://www.kaggle.com/datasets/farzadnekouei/trash-type-image-dataset/)**\n",
    "\n",
    "## Steps\n",
    "* Import the dataset;\n",
    "* Exploration and preprocessing\n",
    "* Compare 3 (or more) pre-trained models\n",
    "* Evaluating the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importig libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner as kt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining general constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "IMAGE_SIZE=(384, 512)\n",
    "EPOCHS=1\n",
    "SEED=10 \n",
    "\n",
    "CURRENT_PATH = os.getcwd()\n",
    "DATASET_PATH = os.path.join(CURRENT_PATH, 'datasets', 'trash_type_dataset')\n",
    "TRAIN_PATH = os.path.join(DATASET_PATH, 'train')\n",
    "VALIDATION_PATH = os.path.join(DATASET_PATH, 'validation')\n",
    "TEST_PATH = os.path.join(DATASET_PATH, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataset(dataset_path: str, batch_size: int = None):\n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(dataset_path,\n",
    "                                                        labels='inferred',\n",
    "                                                        label_mode='int',\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        image_size=IMAGE_SIZE)\n",
    "    return dataset\n",
    "\n",
    "def import_datasets():\n",
    "    print(f\"[INFO] Importing training dataset from {TRAIN_PATH}\")\n",
    "    train_dataset = import_dataset(TRAIN_PATH, BATCH_SIZE)\n",
    "\n",
    "    print(f\"[INFO] Importing validation dataset from {VALIDATION_PATH}\")\n",
    "    validation_dataset = import_dataset(VALIDATION_PATH, BATCH_SIZE)\n",
    "\n",
    "    print(f\"[INFO] Importing test dataset from {TEST_PATH}\")\n",
    "    test_dataset = import_dataset(TEST_PATH, batch_size=None)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset, test_dataset = import_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train_dataset.class_names\n",
    "print(f\"The classes are {classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview of the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_of_dataset(dataset):\n",
    "    image, label = next(iter(dataset))\n",
    "    return image[0].numpy().astype(np.uint8), classes[label[0].numpy()]\n",
    "\n",
    "def plot_n_samples(dataset, n: int, ncols = 4):\n",
    "    samples = []\n",
    "    nrows = (n // ncols) + 1\n",
    "    figure, axis = plt.subplots(ncols=ncols, nrows=nrows, figsize=(12,8))\n",
    "\n",
    "    figure.suptitle(\"Samples - Garbage Image Classification\")\n",
    "\n",
    "    for _ in range(len(samples)):\n",
    "        samples.append(get_sample_of_dataset(dataset))\n",
    "\n",
    "    for ax in axis.reshape(-1):\n",
    "        image, label = get_sample_of_dataset(dataset)\n",
    "        ax.set_title(f\"[class={label}]\")\n",
    "        ax.imshow(image)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.subplots_adjust(wspace=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_n_samples(train_dataset, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: create a new notebook just for exploration step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing experiments\n",
    "For this project, some [available models](https://keras.io/api/applications/#available-models) were chosen to be compared using [Keras](https://keras.io/) framework. Below, it is described each used model:\n",
    "\n",
    "|Model|Size (MB)|Top-1 Accuracy (ImageNet)|\n",
    "|---|---|---|\n",
    "|EfficientNetB7|256|84.3%|\n",
    "|EfficientNetV2S|88|83.9%|\n",
    "|ConvNeXtXLarge|1310|86.7%|\n",
    "\n",
    "Before start the experiments, let's define some configurations and functions in order to re-use funcionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuningModel:\n",
    "    def __init__(self, \n",
    "                 title: str, \n",
    "                 base_model, \n",
    "                 input_shape = IMAGE_SIZE, \n",
    "                 batch_size = BATCH_SIZE):\n",
    "        self.title = title\n",
    "        self.base_model = base_model\n",
    "        self.input_shape = input_shape\n",
    "        self.batch_size = batch_size\n",
    "        self.best_model = None\n",
    "        self._freeze_base_model()\n",
    "\n",
    "    def _freeze_base_model(self):\n",
    "        self.base_model.trainable = False\n",
    "\n",
    "    def _build(self, hp: kt.HyperParameters):\n",
    "        dense_units = hp.Choice('dense_units', values=[256, 128, 64])\n",
    "        \n",
    "        # Conv Layers using a pre-trained model\n",
    "        inputs = tf.keras.Input(shape=(*self.input_shape, 3))\n",
    "        x = self.base_model(inputs, training=False)\n",
    "        \n",
    "        # Dense layers\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        x = tf.keras.layers.Dense(dense_units, activation='relu')(x)\n",
    "        outputs = tf.keras.layers.Dense(len(classes), activation='softmax')(x)\n",
    "        \n",
    "        # Compiling the model\n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "        model = self._compile(model, hp)\n",
    "        return model\n",
    "\n",
    "    def _compile(self, model: tf.keras.Model, hp: kt.HyperParameters):\n",
    "        learning_rate = hp.Choice('learning_rate', values=[0.01, 0.001])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=[tf.keras.metrics.CategoricalAccuracy(name='accuracy')])\n",
    "        return model\n",
    "\n",
    "    def _get_best_model(self, train, validation, best_hps: list):\n",
    "        best_hp = best_hps[0]\n",
    "        model = self._build(best_hp)\n",
    "        model.fit(train, validation_data=validation, epoch=EPOCHS)\n",
    "        return model\n",
    "\n",
    "    def train(self, train: tf.data.Dataset, validation: tf.data.Dataset):\n",
    "        tuner = kt.Hyperband(hypermodel=self._build,\n",
    "                            objective=kt.Objective('val_accuracy', direction='max'),\n",
    "                            overwrite=True,\n",
    "                            directory='checkpoints',\n",
    "                            project_name=self.title,\n",
    "                            max_epochs=EPOCHS)\n",
    "        \n",
    "        tuner.search(train, \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     validation_data=validation)\n",
    "        self.best_model = self._get_best_model(train,\n",
    "                                                validation,\n",
    "                                                tuner.get_best_hyperparameters(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    FineTuningModel('EfficientNetB7', \n",
    "                    tf.keras.applications.EfficientNetB7(include_top=False)),\n",
    "    FineTuningModel('EfficientNetV2S', \n",
    "                    tf.keras.applications.EfficientNetV2S(include_top=False, input_shape=(*IMAGE_SIZE, 3))),\n",
    "    FineTuningModel('ConvNeXtXLarge', \n",
    "                    tf.keras.applications.ConvNeXtXLarge(include_top=False))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in experiments:\n",
    "    experiment.train(train=train_dataset, validation=validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the best experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
